# yaml-language-server: $schema=https://carapace.sh/schemas/command.json
name: wget
description: a non-interactive network retriever
flags:
    --accept-regex=: regex matching accepted URLs
    --ask-password: prompt for passwords
    --auth-no-challenge: send Basic HTTP authentication information without first waiting for the server's challenge
    --backups=: before writing file X, rotate up to N backup files
    --bind-address=: bind to ADDRESS (hostname or IP) on local host
    --body-data=: send STRING as data. --method MUST be set
    --body-file=: send contents of FILE. --method MUST be set
    --ca-certificate=: file with the bundle of CAs
    --ca-directory=: directory where hash list of CAs is stored
    --certificate-type=: client certificate type, PEM or DER
    --certificate=: client certificate file
    --ciphers=: Set the priority string (GnuTLS) or cipher list string (OpenSSL) directly.
    --compression=: choose compression
    --config=: specify config file to use
    --connect-timeout=: set the connect timeout to SECS
    --content-disposition: honor the Content-Disposition header when choosing local file names
    --content-on-error: output the received content on server errors
    --convert-file-only: convert the file part of the URLs only (usually known as the basename)
    --crl-file=: file with bundle of CRLs
    --cut-dirs=: ignore NUMBER remote directory components
    --default-page=: change the default page name
    --delete-after: delete files locally after downloading them
    --dns-timeout=: set the DNS lookup timeout to SECS
    --exclude-domains=: comma-separated list of rejected domains
    --follow-ftp: follow FTP links from HTML documents
    --follow-tags=: comma-separated list of followed HTML tags
    --ftp-password=: set ftp password to PASS
    --ftp-user=: set ftp user to USER
    --ftps-clear-data-connection: cipher the control channel only; all the data will be in plaintext
    --ftps-fallback-to-ftp: fall back to FTP if FTPS is not supported in the target server
    --ftps-implicit: use implicit FTPS (default port is 990)
    --ftps-resume-ssl: resume the SSL/TLS session started in the control connection
    --header*=: insert STRING among the headers
    --hsts-file: path of HSTS database (will override default)
    --http-password=: set http password to PASS
    --http-user=: set http user to USER
    --https-only: only follow secure HTTPS links
    --ignore-case: ignore case when matching files/directories
    --ignore-length: ignore 'Content-Length' header field
    --ignore-tags=: comma-separated list of ignored HTML tags
    --keep-session-cookies: load and save session (non-permanent) cookies
    --limit-rate=: limit download rate to RATE
    --load-cookies=: load cookies from FILE before session
    --local-encoding=: use ENC as the local encoding for IRIs
    --max-redirect: maximum redirections allowed per page
    --method=: use method "HTTPMethod" in the request
    --no-cache: disallow server-cached data
    --no-check-certificate: don't validate the server's certificate
    --no-clobber: skip downloads that would download to existing files
    --no-config: do not read any config file
    --no-cookies: don't use cookies
    --no-directories: don't create directories
    --no-dns-cache: disable caching DNS lookups
    --no-glob: turn off FTP file name globbing
    --no-host-directories: don't create host directories
    --no-hsts: disable HSTS
    --no-http-keep-alive: disable HTTP keep-alive (persistent connections)
    --no-if-modified-since: don't use conditional if-modified-since get requests in timestamping mode
    --no-iri: turn off IRI support
    --no-netrc: don't try to obtain credentials from .netrc
    --no-parent: don't ascend to the parent directory
    --no-passive-ftp: disable the "passive" transfer mode
    --no-proxy: explicitly turn off proxy
    --no-remove-listing: don't remove '.listing' files
    --no-use-server-timestamps: don't set the local file's timestamp by the one on the server
    --no-verbose: turn off verboseness, without being quiet
    --no-warc-compression: do not compress WARC files with GZIP
    --no-warc-digests: do not calculate SHA1 digests
    --no-warc-keep-log: do not store the log file in a WARC record
    --password=: set both ftp and http password to PASS
    --pinnedpubkey=: Public key (PEM/DER) file, or any number of base64 encoded sha256 hashes
    --post-data=: use the POST method; send STRING as the data
    --post-file=: use the POST method; send contents of FILE
    --prefer-family=: connect first to addresses of specified family
    --preserve-permissions: preserve remote file permissions
    --private-key-type=: private key type, PEM or DER
    --private-key=: private key file
    --progress=: select progress gauge type
    --protocol-directories: use protocol name in directories
    --proxy-password=: set PASS as proxy password
    --proxy-user=: set USER as proxy username
    --random-wait: wait from 0.5*WAIT...1.5*WAIT secs between retrievals
    --read-timeout=: set the read timeout to SECS
    --referer=: 'include ''Referer: URL'' header in HTTP request'
    --regex-type=: regex type (posix|pcre)
    --reject-regex=: regex matching rejected URLs
    --rejected-log=: log reasons for URL rejection to FILE
    --remote-encoding=: use ENC as the default remote encoding
    --report-speed=: output bandwidth as TYPE.  TYPE can be bits
    --restrict-file-names=: restrict chars in file names to ones OS allows
    --retr-symlinks: when recursing, get linked-to files (not dir)
    --retry-connrefused: retry even if connection is refused
    --retry-on-http-error=: comma-separated list of HTTP errors to retry
    --save-cookies=: save cookies to FILE after session
    --save-headers: save the HTTP headers to file
    --secure-protocol=: choose secure protocol, one of auto, SSLv2,
    --show-progress: display the progress bar in any verbosity mode
    --spider: don't download anything
    --start-pos=: start downloading from zero-based position OFFSET
    --strict-comments: turn on strict (SGML) handling of HTML comments
    --trust-server-names: use the name specified by the redirection URL's last component
    --unlink: remove file before clobber
    --use-askpass=: specify credential handler for requesting username and password
    --user=: set both ftp and http user to USER
    --waitretry=: wait 1..SECONDS between retries of a retrieval
    --warc-cdx: write CDX index files
    --warc-dedup=: do not store records listed in this CDX file
    --warc-file=: save request/response data to a .warc.gz file
    --warc-header=: insert STRING into the warcinfo record
    --warc-max-size=: set maximum size of WARC files to NUMBER
    --warc-tempdir=: location for temporary files created by the
    --xattr: turn on storage of metadata in extended file attributes
    -4, --inet4-only: connect only to IPv4 addresses
    -6, --inet6-only: connect only to IPv6 addresses
    -A, --accept=: comma-separated list of accepted extensions
    -B, --base=: resolves HTML input-file links relative to URL
    -D, --domains=: comma-separated list of accepted domains
    -E, --adjust-extension: save HTML/CSS documents with proper extensions
    -F, --force-html: treat input file as HTML
    -H, --span-hosts: go to foreign hosts when recursive
    -I, --include-directories=: list of allowed directories
    -K, --backup-converted: before converting file X, back up as X.orig
    -L, --relative: follow relative links only
    -N, --timestamping: don't re-retrieve files unless newer than local
    -O, --output-document=: write documents to FILE
    -P, --directory-prefix=: save files to PREFIX/..
    -Q, --quota=: set retrieval quota to NUMBER
    -R, --reject=: comma-separated list of rejected extensions
    -S, --server-response: print server response
    -T, --timeout=: set all timeout values to SECONDS
    -U, --user-agent=: identify as AGENT instead of Wget/VERSION
    -V, --version: display the version of Wget and exit
    -X, --exclude-directories=: list of excluded directories
    -a, --append-output=: append messages to FILE
    -b, --background: go to background after startup
    -c, --continue: resume getting a partially-downloaded file
    -d, --debug: print lots of debugging information
    -e, --execute=: execute a `.wgetrc'-style command
    -h, --help: print this help
    -i, --input-file=: download URLs found in local or external FILE
    -k, --convert-links: make links in downloaded HTML or CSS point to local files
    -l, --level=: maximum recursion depth (inf or 0 for infinite)
    -m, --mirror: shortcut for -N -r -l inf --no-remove-listing
    -o, --output-file=: log messages to FILE
    -p, --page-requisites: get all images, etc. needed to display HTML page
    -q, --quiet: quiet (no output)
    -r, --recursive: specify recursive download
    -t, --tries=: set number of retries to NUMBER (0 unlimits)
    -v, --verbose: be verbose (this is the default)
    -w, --wait=: wait SECONDS between retrievals
    -x, --force-directories: force creation of directories
documentation:
    command: '[AI] `wget` is a popular command-line tool used for downloading files from the web. It supports multiple recursive downloads using FTP and HTTP protocols, and it can resume interrupted downloads. Users can provide a URL as an argument to download a file or a whole directory with its contents. Additional features include authentication, cookies support, and conversion of links in HTML files to local copies.'
    flag:
        accept: '[AI] The `--accept` option in `wget`, a command-line download tool, specifies a list of file name patterns for accepted files. Only files with matching filenames will be saved locally during a download. This helps to control what types or names of files are retrieved from a server during an automated download process.'
        accept-regex: '[AI] `wget` is a command-line utility used to download files from the web. The `--accept-regex` option is a feature that allows you to specify regex patterns for selecting the files to be downloaded based on their names or URLs. When this option is used, only files with names or URLs matching the specified regex pattern will be downloaded. This can help in automating the process of downloading specific types or versions of files from a website.'
        adjust-extension: |-
            [AI] `wget` is a popular command-line tool for downloading files from the web. The option `--adjust-extension` (or `-A`) is used to automatically change file extensions during download based on the server's content type.

            When you specify this option, wget will rename the downloaded file according to the content type returned by the server. For instance, if a file with MIME type "image/jpeg" is being downloaded and the original filename is "data.txt", then wget will save it as "data.jpg".

            This can be helpful when dealing with web services or servers where filenames may not be provided or may not accurately represent the content of the file being downloaded.
        append-output: '[AI] `wget --append-output=filename` is a command-line option used with the WGET utility to append the output of multiple downloads to a single file named "filename". Instead of creating a new file for each downloaded content, this option allows merging all the contents into one file.'
        ask-password: '[AI] The `wget` command with the option `--ask-password` is used to download files from the internet using the WGET utility. With this option, WGET will prompt for a password when it encounters an HTTP or FTP site requiring authentication before allowing file transfer. This helps secure data by ensuring that only authorized users can access and download the files.'
        auth-no-challenge: '[AI] The `wget` command is a popular utility for downloading files from the web using the command line. The option `--auth-no-challenge` is used to disable challenges for basic authentication. In other words, it tells `wget` not to ask for a username and password if they are required to access the protected resource. This can be useful when the authentication information is provided elsewhere or when automating the download process from a script. However, using this option without proper security measures may expose sensitive information and should be used with caution.'
        background: '[AI] The `wget` command with the `--background` option is used to download files in the background mode, meaning that the command will be executed as a separate process and will not block the terminal or the script where it is called from. It allows other commands or scripts to continue running concurrently. The progress of the download will still be displayed on the terminal if `--quiet` is not used.'
        backup-converted: '[AI] `wget` is a command-line tool used to download files from the web. The option `--backup-converted` is used to keep backup copies of files that have been successfully downloaded and have been converted to match the local file name and permissions. When this option is used, wget will rename the original file with a timestamp suffix before downloading the new file, and move the old file to a `.backup` directory if it already exists. If no such directory exists in the current directory, wget will create one automatically. This helps ensure that important files are not overwritten accidentally during the download process.'
        backups: '[AI] `wget --backups` is a command-line option for the Wget downloader tool. It instructs Wget to keep backup files of previously downloaded files, with the backup file having the same name as the original file but with a tilde (~) added at the beginning. This can be useful in cases where you want to revert to a previous version of a downloaded file without having to download it again from the source.'
        base: '[AI] `wget --base=<URL>` is a command-line option used with the WGET downloader tool. It sets the base URL for all links being downloaded, allowing you to navigate through directories or sites without having to manually specify the full path for each file. This can be useful when downloading an entire website or multiple files from a site with interconnected links.'
        bind-address: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--bind-address <IP_ADDRESS>` allows users to specify an IP address or hostname to bind `wget` to, making it accessible over a network instead of just on localhost by default. This can be useful when running `wget` in a server environment where you want to download files remotely without interfering with local traffic.'
        body-data: '[AI] `wget --body-data` is a command-line option used with the wget utility, which is a popular downloader for files over the web. This option is used to send custom HTTP requests with a body (data) along with the GET request. The body data is specified using the `--post-data` or `--body-file` options for non-text data, and `--header "Content-Type: text/plain" --body-data="<data>"` for text data. This can be useful for interacting with web services, APIs, or forms that require user input.'
        body-file: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--body-file=filename` instructs `wget` to save the response body of an HTTP or FTP request directly into the specified file, instead of saving the whole page as HTML with the default operation. This can be useful when you only need specific data returned by a server and want to avoid downloading unnecessary HTML content.'
        ca-certificate: '[AI] The `--ca-certificate` option in the `wget` command is used to specify a custom SSL certificate file for verification of secure connections. This can be useful when accessing websites with self-signed certificates or when using proxies that have their own certificates. The specified file should contain the root certificate authority (CA) key used to sign the SSL certificate of the website being accessed, allowing `wget` to trust the connection and proceed with the download.'
        ca-directory: '[AI] The `--ca-directory` option in the `wget` command is used to specify a directory containing SSL certificates for secure (HTTPS) downloads. This allows wget to verify the authenticity of the SSL certificates presented by the remote server during the download process, enhancing security and preventing potential man-in-the-middle attacks.'
        certificate: '[AI] The `wget` command is a popular utility for downloading files from the web using the HTTP and FTP protocols. The option `--certificate` is used to specify a SSL certificate file for secure (HTTPS) downloads. This option helps wget authenticate itself when required, allowing the download to proceed. The certificate file should be in PEM format and includes both the private key and the certificate. In summary, `wget --certificate` is an option used to enable HTTPS downloads with client-side certificate authentication.'
        certificate-type: |-
            [AI] `wget` is a command-line tool for downloading files from the web. The option `--certificate-type <type>` is used to specify the type of SSL certificate `wget` should use when making secure HTTPS connections.

            The SSL certificate determines the identity of the server being connected to, and helps ensure data integrity and confidentiality during the transfer. The certificate type can be specified as "client" or "ca", depending on whether it is a client-side or CA (Certifying Authority) certificate that will be used for verification.

            Using `--certificate-type client` directs `wget` to use a client-side SSL certificate file when establishing the connection, while using `--certificate-type ca` instructs `wget` to make use of a CA certificate file for trusting or verifying the server's identity.
        ciphers: '[AI] The `--ciphers` option in `wget`, a popular command-line tool for downloading files from the web, specifies which SSL/TLS cipher suites to use during secure (HTTPS) connections. Cipher suites define the encryption algorithms and key exchange methods used to secure the communication between the client and server. By using specific ciphers, you can enhance security by excluding older or less secure options. For example, `--ciphers TLSv1.2:TLS_AES_256_GCM_SHA384` would force wget to use only the most secure SSL/TLS version (TLSv1.2) and AES-256-GCM with SHA384 encryption for secure connections.'
        compression: '[AI] `wget --compression` is a command-line option used with the file retrieval utility, wget. This option enables compressed transfer mode for downloading files. Compressed files are indicated by extensions such as .gz, .zip or .bz2. With this option, wget automatically decompresses the downloaded file upon completion of the transfer. Therefore, using `--compression` saves a step in the process and results in having the uncompressed file ready for use after the download is finished.'
        config: '[AI] `wget --config` is a command-line option used with the file downloading tool `wget`. It allows users to specify a custom configuration file containing various `wget` settings, such as output directory, user agent string, timeouts, and more. Using this option, users can override the default behavior of `wget` and customize it according to their specific requirements. The specified configuration file should be in INI format.'
        connect-timeout: '[AI] `wget` is a command-line tool used to download files from the web. The `--connect-timeout` option sets the number of seconds `wget` waits for a connection to be established before giving up and producing an error. If the connection is not made within the specified time, the download will fail. This option helps prevent `wget` from hanging indefinitely when it''s unable to establish a connection to a server.'
        content-disposition: '[AI] The `--content-disposition` option in the `wget` command is used to download files with specified content types and adjust the local file name based on the Content-Disposition header received in the HTTP response. This header provides information about the location, media type, and other attributes of the attachment being downloaded. Using this option ensures that the correct file names are saved locally when downloading files like HTML pages with linked resources or binary files.'
        content-on-error: '[AI] The `--content-on-error` option in `wget`, a popular command-line tool for downloading files from the web, instructs it to save any partial content of a failed download to a file with a "_resume.part" extension in the same directory as the original download request. This allows users to resume interrupted or aborted downloads later without starting over from the beginning.'
        continue: '[AI] `wget --continue` is a command-line option used with the file retrieval utility ''wget''. It is used to resume interrupted or failed downloads. When this option is specified, wget will check if the file being downloaded already exists and if it is incomplete, it will continue the download from the point of failure instead of starting over. This can save time when downloading large files.'
        convert-file-only: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--convert-file-only` tells `wget` to only convert file names and extensions during the download process, without actually downloading the files themselves. This can be useful when dealing with large numbers of URLs, where you only want to update the file name list, while avoiding unnecessary file transfers or storage.'
        convert-links: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--convert-links` tells `wget` to recursively follow links in HTML and XML files and convert them into local links pointing to the downloaded versions of those files, instead of the original remote URLs. This can be useful when downloading an entire website or a webpage that contains multiple linked resources, allowing you to keep a self-contained copy with all the content in one place.'
        crl-file: '[AI] `wget` is a command-line utility for downloading files from the web. The option `--certificate-file <file>` or `--crl-file <file>` is used with `wget` to specify a file containing X.509 certificates or Certificate Revocation Lists (CRLs), respectively, for SSL/TLS certificate verification during secure HTTPS downloads. This helps ensure that the server''s certificate is trusted and has not been revoked.'
        cut-dirs: '[AI] The `--cut-dirs` option in the `wget` command is used to remove directories from the output file names during a download. It shortens the path prefixes, keeping only the base names of files. This can be useful when dealing with deep directory structures or when wanting to keep a clean file name list for further processing.'
        debug: '[AI] `wget --debug` is a command-line option used with the WGET utility to download files from the web. The `--debug` flag enables verbose output, meaning it provides detailed information about the download process. This includes displaying HTTP headers, progress meters, and error messages, among other things. It helps in troubleshooting and understanding the flow of data during the file transfer.'
        default-page: '[AI] `wget` is a command-line tool for downloading files from the web using the Hypertext Transfer Protocol (HTTP) or other protocols. The option `--default-page` specifies the default index file name to use when downloading an entire website using the `-r` (recursive) option. If not specified, `wget` assumes "index.html" as the default file name for each directory it downloads. For instance, `wget --recursive --no-clobber --level=2 --domains example.com --default-page myfile.php example.com/` would download files up to level 2 from "example.com" domain, but with the specified file name "myfile.php" as the default page for each directory.'
        delete-after: '[AI] The `wget` command is a popular tool for downloading files from the web using the command line. The option `--delete-after` is used to delete the local file after it has been successfully downloaded. In other words, it automates the process of downloading and deleting a file in one step. This can be useful when dealing with large or temporary files that are not needed after they have been downloaded.'
        directory-prefix: '[AI] The `--directory-prefix` option in the `wget` command is used to specify a prefix that will be added as a subdirectory to the base directory for all downloaded files. This can be useful when downloading multiple files from different URLs, as it allows you to keep them organized by keeping them all in one subdirectory instead of mixing them with other files in your current working directory. For example, `wget --directory-prefix=downloads/ http://example.com/*` will download all the files from `http://example.com/` and save them in a subdirectory named "downloads" within your current working directory.'
        dns-timeout: '[AI] `wget --dns-timeout` is an option used with the `wget` command in Unix-based systems for downloading files from the web using the GET method. The `--dns-timeout` option sets a timeout for DNS lookups, i.e., the time wget waits for the DNS server to respond before considering the lookup as failed. If the DNS lookup fails repeatedly, it can prevent successful downloads due to network issues or unresponsive servers. By setting an appropriate value with `--dns-timeout`, users can improve the robustness and reliability of their wget downloads.'
        domains: '[AI] The `--domains` option in `wget`, a command-line tool for downloading files from the web, is used to restrict the websites from which files are retrieved to those with a specified domain name. When used with an argument, `wget` will only download content from websites with that specific domain. For example, using `--domains example.com` will cause `wget` to download files only from sites with the ".example.com" top-level domain. This option can be useful for limiting downloads to trusted sources or for automating the retrieval of data from multiple related websites.'
        exclude-directories: '[AI] The `--exclude-directories` option in the `wget` command is used to exclude directories or subdirectories during a download. It prevents the recursive descent into specified directories and only downloads files that are located directly under the given URL. This can help save bandwidth and disk space by excluding unnecessary directories from the download process.'
        exclude-domains: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--exclude-domains` allows users to exclude specific domains or subdomains while downloading files using wget. This means that files will be retrieved only from domains or subdomains not included in the list provided with this option. Multiple domains or subdomains can be excluded by separating them with commas. For example, `wget --recursive --exclude-domains=example1.com,example2.org https://www.startingdomain.com` will download files from `https://www.startingdomain.com` and its subdomains, excluding files from `example1.com` and `example2.org`.'
        execute: '[AI] `wget --execute` is a feature in the WGET downloader tool that allows users to specify a list of commands or actions to be executed after successfully downloading each file. The command list is provided using a file in the INI format, with each action defined under its respective URL in the file. This functionality can be used for automating post-download tasks, such as extracting archives or running scripts on the downloaded files.'
        follow-ftp: '[AI] The `wget` command with the option `--follow-ftp` is used to download files from FTP (File Transfer Protocol) servers in a recursive manner. When you use this option, `wget` will not only download the specified file but also follow the links contained within that file and download them as well if they are FTP links. This can be useful when trying to mirror an entire FTP site or download multiple files from different locations that are linked together on a single FTP server.'
        follow-tags: '[AI] `wget --follow-tags` is a command-line option used with the file retrieval tool, `wget`. This option instructs `wget` to download not only the files linked directly in the HTML document but also those linked from those files, recursively following links with specified tags. For example, `--follow-tags html,css` will download all HTML and CSS files referenced in the starting HTML document and any subsequent files linked from them. This can be useful for downloading an entire website or a specific part of it that relies on multiple linked resources.'
        force-directories: '[AI] The `wget` command is a popular tool for downloading files from the web using the command line. The option `--force-directories` (or just `-r`) is used to enable recursive downloading with wget. This means that wget will not only download the file you specify, but also any files or directories referenced within that file, creating the necessary directories on your local machine as needed. The `--force-directories` option ensures that the directory structure of the remote content is preserved during the download process.'
        force-html: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--force-html` is used to force wget to download a file with a ".html" or ".htm" extension even if the server returns a MIME type other than HTML. In other words, it ensures that wget saves files with HTML extensions even if they are served as text/plain or application/octet-stream by the web server. This can be useful when dealing with websites that do not correctly set their Content-Type headers.'
        ftp-password: '[AI] The `wget` is a command-line tool used to download files from the internet. The option `--ftp-password` is used to provide the password for an FTP (File Transfer Protocol) authentication when downloading a file using `wget`. This option ensures that the password is not displayed during the transfer process by hiding it with an encrypted method, adding the password to the .wget-password file in your home directory. It''s essential to keep this file secure since it contains sensitive information.'
        ftp-user: '[AI] The `--ftp-user` option in the `wget` command is used to specify the FTP username when downloading files using FTP protocol. This option helps to authenticate and identify the user account for accessing remote FTP servers securely. By providing the correct FTP username, `wget` can successfully transfer the required files from the server to the local system.'
        ftps-clear-data-connection: '[AI] The `--ftps-clear-data-connection` option in the `wget` command is used to clear the FTP data connection before starting a new transfer. This can be useful when dealing with FTP servers that don''t support resuming partially transferred files, or when you want to ensure that no old data remains on the connection that could cause issues with subsequent transfers. By default, `wget` keeps the FTP data connection open between transfers to improve performance for large files, but this option allows you to disable that behavior and start each transfer with a fresh connection.'
        ftps-fallback-to-ftp: '[AI] The `--ftps-fallback-to-ftp` option in the `wget` command is used to specify that if an FTPS (FTP Secure) transfer fails, `wget` should attempt to use plain FTP instead. This can be useful when connecting to servers that do not fully support FTPS protocol or when the server certificate cannot be verified. With this option, the data transfer continues despite potential security concerns with the fallback to FTP. It''s important to note that using FTP instead of FTPS could expose your data in transit, so it should only be used as a last resort.'
        ftps-implicit: '[AI] The `--ftp-implicit` option in `wget`, a popular command-line downloader, indicates that FTP (File Transfer Protocol) is the assumed protocol for any given URL if it does not start with an HTTP or HTTPS scheme. This means that when you provide a URL without a specific protocol indicator, `wget` will assume FTP and attempt to download the file using FTP instead of HTTP or HTTPS. This can be useful in certain situations where you''re sure the target URL is an FTP link but have neglected to add the ''ftp://'' scheme.'
        ftps-resume-ssl: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--ftps-resume-ssl` is used when downloading files over FTPS (File Transfer Protocol Secure) and it enables resuming SSL connections. This means that if an SSL-secured FTP transfer is interrupted, `wget` can automatically restart the download from the point of interruption without having to reestablish the entire SSL connection.'
        header: '[AI] The `--header` option in `wget` is used to send custom HTTP headers along with the request. This can be useful for various purposes such as authentication, setting content types or overriding caching directives. The header is provided as a key-value pair using the `"--header=Key: Value"` syntax. Multiple headers can be specified by chaining multiple options together. For example, `--header="Authorization: Bearer mytoken" --header="Accept: application/json"`.'
        help: '[AI] `wget --help` is a command used in Unix-based systems to display detailed help information about the GNU Wget utility. It''s a versatile tool for downloading files from the web using various retrieval methods, such as HTTP and FTP. The `--help` option provides an extensive list of available options, usage instructions, and brief descriptions, allowing users to effectively utilize this command for their file transfer needs.'
        hsts-file: '[AI] `wget` is a command-line tool for downloading files from the web using various protocols, including HTTP and HTTPS. The option `--save-history=FILE` or shorthand `--hstfile=FILE`, sets the file `FILE` as the History File for wget''s HTTP state tracking (also known as persistent cookies). This means that wget will save the session data to this file, and use it for subsequent downloads to avoid re-downloading files or re-entering authentication credentials. The session data includes visited URLs, cookies, and other HTTP headers.'
        http-password: '[AI] `wget` is a command-line tool used to download files from the web. The option `--http-password` is used to provide a password for an HTTP or HTTPS authentication request. This option is followed by the password itself, enclosed in quotes if it contains spaces. For example: `wget --no-clobber --quiet --http-user=username --http-password=mypassword http://example.com/file.html`'
        http-user: '[AI] `--http-user` is an option used with the command-line tool `wget`, which is designed for downloading files from the web. This option sets the custom HTTP username to be used during authentication when accessing protected servers or websites requiring user credentials. By providing a username with this flag, wget can successfully complete the login process and proceed with downloading the required files.'
        https-only: '[AI] The `wget` command is a popular utility for downloading files from the web using the HTTP or HTTPS protocols. The option `--https-only` instructs `wget` to only download files using HTTPS, which ensures that all data exchanged between the client (the `wget` command) and the server is encrypted. This can be useful when prioritizing secure connections or accessing websites that exclusively use HTTPS.'
        ignore-case: '[AI] `wget --ignore-case` is a command-line option used with the download utility ''wget''. This option modifies the default behavior of wget by making the file matching case insensitive during the file name resolution process. With this option, wget will match files with names that differ only in capital letters. For example, it would download both "file.txt" and "File.txt" to the same destination if specified in the URL or local file path.'
        ignore-length: '[AI] `wget --ignore-length` is an option used with the command-line tool `wget` for downloading files from the web. It tells `wget` to ignore the Content-Length header in the HTTP response and proceed with the download, even if the reported size differs from the expected one. This can be useful when dealing with potentially misconfigured servers or broken downloads.'
        ignore-tags: '[AI] `wget --ignore-tags` is a command-line option used with the Wget download tool. It tells Wget to ignore specified HTML tags while downloading web pages, allowing users to exclude scripts, stylesheets, or other elements that might not be necessary for their purpose, reducing bandwidth usage and download time. The ignored tags are specified using regular expressions, e.g., `--ignore-tags="script,style"`. This option can help minimize the amount of data transferred while still obtaining the main content of a webpage.'
        include-directories: '[AI] The `--include-directories` option in the `wget` command is used to download files from a website, including those located in specified directories and their subdirectories. By default, wget only downloads files mentioned in the URL or listed in the Robots Exclusion Standard file (robot.txt). This flag allows extending the list of files to be downloaded by including directories and recursively downloading files within them. It takes a list of directories as an argument, separated by colons on Unix-based systems or semicolons on Windows.'
        inet4-only: '[AI] `wget` is a command-line tool for downloading files from the internet using the Hypertext Transfer Protocol (HTTP) and other related protocols. The option `--inet4-only` is used to restrict `wget` to only use Internet Protocol version 4 (IPv4) addresses when resolving domain names. This can be useful in certain network environments where IPv6 is not supported or is restricted for security reasons.'
        inet6-only: '[AI] The `wget` command is a popular line-oriented file retrieval tool used to download files from the internet. The option `--inet6-only` tells `wget` to use IPv6 addresses exclusively when resolving domains and establishing connections. This means that `wget` will prioritize connecting to servers using IPv6 addresses over IPv4 addresses, helping to promote the use of IPv6 in the internet infrastructure.'
        input-file: |-
            [AI] The `wget` command is a popular line command used for downloading files from the web. The option `--input-file` tells `wget` to read a list of URLs to download from a file instead of specifying the URL directly on the command line.

            So, if you have a text file containing a list of URLs to be downloaded, you can pass that file to `wget` using the `--input-file` option as follows:

            ```
            wget --input-file=myfile.txt
            ```

            In this example, `myfile.txt` is the name of the input file containing the list of URLs for the files you want to download. The `wget` command will then read each URL from that file and download the corresponding files accordingly.
        keep-session-cookies: '[AI] `wget` is a command-line tool for downloading files from the web using HTTP and FTP protocols. The option `--keep-session-cookies` instructs `wget` to save and use cookies during the download process. Cookies are small pieces of data stored on your computer that help maintain session information between your browser and a server. This option ensures that any cookie data is kept across multiple downloads or retries, preventing you from having to re-enter login credentials or other session information for websites requiring authentication.'
        level: '[AI] The `--level` option in `wget`, a command-line download tool, specifies a recursion depth for following links and downloading associated files. A depth of 0 means only the initial URL will be fetched, while a positive number indicates that linked pages up to that level will also be downloaded. For example, using `--level=2` would download the specified page and any pages linked from it, but not the pages linked within those pages.'
        limit-rate: '[AI] `wget --limit-rate` is a command-line option used with the WGET utility to download files from the web with a specified maximum download speed. The option takes the argument in the format of "Nk" where N is the number and k represents kilobytes (KB), megabytes (MB), or gigabytes (GB). For example, `--limit-rate=10M` sets the download rate limit to 10 Megabytes per second. This option helps manage bandwidth usage during large-scale file downloads and can be useful when working with slow internet connections or when multiple processes need to share a limited connection.'
        load-cookies: '[AI] `wget --load-cookies` is a command-line option used with the Wget downloader tool. It is utilized to load cookies from a file into a new Wget session, allowing the tool to recreate and include previously stored HTTP cookies in subsequent download requests. This functionality helps maintain user session information across multiple downloads or retrievals of web pages that use cookies for authentication or personalization purposes.'
        local-encoding: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--local-encoding=ENCODING` specifies the local encoding to use when saving text files from remote servers. This can be important when dealing with websites that use non-standard character encodings, as it ensures that text files are saved correctly. ENCODING is a string representing the desired encoding, such as "utf-8" or "iso-8859-1".'
        max-redirect: '[AI] The `--max-redirect` option in `wget`, a command-line tool for downloading files from the web, sets a limit on the number of redirections that will be followed during the download process. By default, wget follows redirects until the final location is reached, but with this option, you can specify a maximum number of allowed redirections. This helps prevent endless loops or infinite redirections, ensuring a successful and efficient download.'
        method: '[AI] `wget` is a command-line utility used to download files from the internet. The `--method` option is not a standard option for `wget`. It''s an extension provided by some advanced implementations like `GNU Wget`. This option allows specifying custom HTTP methods, such as `HEAD`, `PUT`, or `DELETE`, instead of just the GET method used by default. Using this option, users can perform various actions other than simple downloading with `wget`.'
        mirror: '[AI] `wget --mirror` is a command-line option used with the file retrieval utility, `wget`. This option initiates a recursive download of an entire website or a specific directory on that website, following all hyperlinks within the starting URL. The downloaded files are saved in local directories according to their original site structure, preserving file permissions and timestamps as closely as possible. This is often referred to as a "mirror" of the original website.'
        no-cache: '[AI] The `wget` command is a popular tool for downloading files from the web. The option `--no-cache` tells `wget` not to request and save metadata files (like `.htm` or `.html` files in the case of HTML pages) during the download process, which are usually used by web browsers to cache webpages for faster access in future visits. With this option enabled, `wget` will only download the actual content files and not their associated metadata.'
        no-check-certificate: '[AI] `wget --no-check-certificate` is a command-line option used with the WGET utility to download files from the internet. The `--no-check-certificate` option disables certificate verification during SSL communication, allowing data to be transferred even if the certificate presented by the server does not match or is invalid. This can bypass security measures intended to prevent connections to potentially malicious servers and should only be used with trusted sites or after verifying their certificates manually.'
        no-clobber: '[AI] `wget --no-clobber` is a command-line option used with the Wget downloader tool. It instructs Wget not to overwrite an already existing local file when downloading a new one, preventing data loss or incorrect file modifications. If a local file has the same name as the remote file being downloaded, Wget will rename the local file by appending "_" and a number to make it unique, allowing the new download to be saved.'
        no-config: '[AI] The `wget` command is a popular utility used to download files from the internet using URLs as arguments. The option `--no-config` tells `wget` not to load or save any custom configuration file before or after running the command. This means that only the default settings of `wget` will be used for this particular command execution.'
        no-cookies: '[AI] The `wget` command with the option `--no-cookies` instructs the tool not to send or save cookies during a web download or retrieval process. Cookies are small data files stored on a user''s computer by a web server to maintain session information, personalization, and other preferences. By disabling cookies, wget does not include these files in its requests or store them locally. This can be useful for various reasons such as preventing tracking, maintaining privacy, or testing websites without saved data.'
        no-directories: '[AI] `wget` is a command-line tool for downloading files from the web using HTTP and FTP protocols. The option `--no-directories` (or `-nR`) tells `wget` not to create directories (automatically follow links into subdirectories) while downloading files. In other words, it sets the recursion depth to 0, meaning that only the file specified will be downloaded and no further directories or files within that directory will be processed. This can be useful when you only want to download a specific file from a server without following any links into subdirectories.'
        no-dns-cache: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--no-dns-cache` tells wget not to use the local DNS cache when resolving domain names. This means that for each URL, wget will perform a fresh lookup of the IP address using the DNS server instead of using any previously cached results. This can be useful in cases where you want to ensure that you are getting the most up-to-date IP addresses for the domains in question.'
        no-glob: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--no-glob` disables globbing behavior in file name patterns given to `wget`. Globbing is a Unix feature that allows expanding certain characters in a string, such as `*` or `?`, into multiple filenames. With `--no-glob`, these patterns are treated as literal strings and downloaded as-is. This can be useful when you want to download specific files with exact names without any wildcard expansion.'
        no-host-directories: '[AI] The `wget` command is a popular tool for downloading files from the web. The option `--no-host-directories` tells `wget` not to create subdirectories under the current directory based on the hostnames in the URLs being downloaded. Instead, it will save all files in the current working directory. This can be useful if you want to download multiple files from different hosts into a single directory without creating new directories for each host.'
        no-hsts: '[AI] The `wget` command with the option `--no-hosts-lookup` (shorthand: `--no-hsts`) disables the Hosts file lookup in Wget. This means that Wget will not check the system''s hosts file during the download process, effectively bypassing any DNS blocking or redirection rules set up in the hosts file. It is useful for downloading content from websites whose addresses might be blocked or redirected on your local system due to security reasons or other filters.'
        no-http-keep-alive: '[AI] `wget` is a command-line tool for downloading files from the web using the HTTP and FTP protocols. The option `--no-http-keep-alive` tells wget not to use HTTP keep-alive feature during the transfer. This means that after each file download, a new HTTP connection will be established instead of reusing an existing one with the same server. This can result in increased network overhead and potentially slower download times for multiple files from the same server if other factors are equal. However, this option may be useful in certain situations where maintaining a persistent connection is not desirable, such as when dealing with unstable connections or large number of parallel downloads.'
        no-if-modified-since: '[AI] `wget --no-if-modified-since` is a command-line option used with the WGET downloader tool. This option disables the conditional request based on the Last-Modified header, which is typically used to check if a file has been modified since last downloaded before resuming or downloading it again. By using `--no-if-modified-since`, WGET will always download the file from the server without checking its modification time, which can be useful when the server does not accurately report file modification times or when downloading multiple files in a script with no need for conditional requests.'
        no-iri: '[AI] `wget` is a command-line tool for downloading files from the web using the Hypertext Transfer Protocol (HTTP). The option `--no-iri` disables the handling of Internationalized Resource Identifiers (IRIs) in the URLs being downloaded. IRIs support schemes, authority parts, and paths with unicode characters or internationalized domains. Disabling this option can speed up downloads for simple URLs or prevent issues when dealing with specific web servers that do not properly handle IRIs.'
        no-netrc: '[AI] `wget` is a command-line tool for downloading files from the web using HTTP, FTP, and other protocols. The option `--no-netrc` tells `wget` not to use or write to the `.netrc` file for handling default authentication credentials for accessing remote servers. This can be useful when you don''t want to store or expose your login information in plaintext files on your local machine.'
        no-parent: '[AI] The `--no-parent` option in the `wget` command is used to prevent wget from climbing up directories during a recursive download. In other words, it tells wget not to follow symbolic links or change to parent directories when downloading files or following links. This can be useful when you only want to download files from a specific directory without affecting the hierarchy of your local directory structure.'
        no-passive-ftp: |-
            [AI] The `wget` command is a popular utility used for downloading files from the internet using the HTTP, FTP, and other protocols. The option `--no-passive-ftp` is used with the FTP (File Transfer Protocol) transfers in `wget`.

            By default, when `wget` initiates an FTP transfer, it operates in passive mode, meaning that it waits for the server to contact it on a random high port number for data transfer. However, some restrictive firewalls or routers may not allow incoming connections from the server on these high port numbers, causing download failures.

            The `--no-passive-ftp` option instructs `wget` to use active FTP mode instead. In this mode, `wget` opens a connection to the server's data port (usually 21) and initiates the data transfer itself without waiting for the server to contact it. This can help bypass such restrictions and successfully complete the download.
        no-proxy: '[AI] `wget` is a command-line tool for downloading files from the web using HTTP, FTP, and other protocols. The option `--no-proxy` tells `wget` not to use any proxy server during the download process. This means that all requests will be made directly to the remote servers without going through an intermediary proxy server. This can bypass some restrictive firewalls or proxies and is useful when you need to download files from a remote server that does not allow access through a proxy.'
        no-remove-listing: '[AI] The `wget` command is a popular line tool for downloading files from the web. The option `--no-remove-listing` is used to prevent `wget` from deleting the listing of files that are being downloaded during the process. In other words, it keeps the list of files in the terminal window as they are being downloaded instead of removing it after each file has been transferred. This can be helpful for keeping track of the progress of multiple downloads or large transfers where the output is not displayed on the terminal by default.'
        no-use-server-timestamps: '[AI] The `--no-use-server-timestamps` option in `wget`, a command-line tool for downloading files from the web, instructs it to not use remote file timestamps when determining the local modification time of downloaded files. Instead, wget will use the current local modification time, effectively overwriting the remote timestamp with the local one. This can be useful when you want to ensure that locally downloaded files have consistent timestamps and are not accidentally overwritten by older versions due to remote file timestamp differences.'
        no-verbose: '[AI] `wget` is a command-line tool for downloading files from the web using the HTTP and FTP protocols. The option `--no-verbose` (or simply `-q`) instructs `wget` to run in quiet mode, meaning it will not print progress messages or other detailed information during the download process. This can be useful when running multiple downloads at once or when automating download tasks with scripts.'
        no-warc-compression: |-
            [AI] `wget` is a popular command-line tool for downloading files from the web. The option `--no-warc-compression` disables Warc (Web ARChive) compression during the download process.

            Warc is an archival format used to store and index large amounts of data, including web pages and other resources. When enabled, `wget` can automatically use Warc compression to save bandwidth and storage space by downloading multiple related files as a single compressed archive. However, some users might not need or want this feature, especially when dealing with small files or specific file types.

            The `--no-warc-compression` option disables Warc compression, forcing `wget` to save each file separately instead of compressing them into an archive. This can be useful for specific use cases where individual files need to be accessed directly without the need for additional compression or archiving.
        no-warc-digests: '[AI] The `wget` command is a popular utility for downloading files from the web using the HTTP, FTP, and other protocols. The option `--no-warc-digests` is used to disable the verification of WARC (Web Archive) file digests during downloads with the `--save-files` or `--save-directory` options. By disabling this feature, you may sacrifice data integrity checks but potentially save time as digest calculations can be resource-intensive.'
        no-warc-keep-log: '[AI] The `wget` command with the option `--no-warc-keep-log` instructs the Wget downloader tool not to keep Warc logs during the HTTP or FTP retrieval process. Warc logs are an archive format for web data, used primarily for web archiving and indexing purposes. By using this option, you''re telling Wget to discard any Warc logs that would normally be generated during the download process.'
        output-document: '[AI] The `wget` command is a popular tool for downloading files from the web. The option `--output-document <filename>` tells `wget` to save the downloaded content into the specified file instead of displaying it in the terminal or browsing it in the default application. For instance, `wget --output-document=file.html http://example.com` will download the HTML content from `http://example.com` and save it as `file.html`.'
        output-file: '[AI] The `wget` command is a popular tool for downloading files from the web using URLs as arguments. The option `--output-file <filename>` tells `wget` to save the downloaded file with the specified name instead of using the original one or saving it to the default location in the current directory. For example, `wget --output-file=myfile.txt http://example.com/myfile.html` will download a file from `http://example.com/myfile.html` and save it as `myfile.txt`.'
        page-requisites: '[AI] `wget --page-requisites` is a command used with the Wget downloader tool. This option instructs Wget to download not only the requested webpage but also all the dependent files such as images, stylesheets, and JavaScript files that are necessary for rendering the page correctly. In summary, it retrieves the whole website including its components, making it useful when you want to obtain a complete copy of a website.'
        password: |-
            [AI] The `--password` option in the `wget` command is used to specify a password for an HTTP or FTP authentication request. When accessing protected resources over the network, this option ensures that the password is securely transmitted during the download process. It takes the password as its argument. For example:

            ```bash
            wget --password=mypassword https://user:mypassword@example.com/protected_file.html
            ```
        pinnedpubkey: '[AI] The `--pinned-pubkey` option in the `wget` command is used to specify a particular GPG key for signing a specific repository or package. This key is then trusted and used for verifying the authenticity of downloaded packages or files. The goal is to ensure that the data comes from the expected source and has not been tampered with during transmission. The public key file should be in OpenPGP format, and its contents are compared against the GPG key''s fingerprint that is associated with the repository or package. This feature helps maintain security and integrity by using a trusted key for downloading data.'
        post-data: '[AI] `wget --post-data` is a command-line option for the wget utility, which is a popular command-line downloader in Unix-based systems. This option allows sending custom data with an HTTP POST request instead of just GET requests. The value following `--post-data` is the data to be sent as the body of the POST request. It can be either provided directly in the command line, or via a file using `@filename`. The data is typically in key=value format for form submissions.'
        post-file: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--post-file` is used to submit an HTTP request with a file attached as form data. This can be useful when interacting with websites that require form submissions with attachments, such as file uploads or account registration. The specified file is read and sent as part of the POST request.'
        prefer-family: '[AI] The `--prefer-family` option in `wget` is used to prefer a specific type of protocol family when downloading files. This can be useful when dealing with mirrors or proxies that support multiple protocols, such as HTTP and FTP. By using this option, you can instruct `wget` to prioritize one protocol over another. For example, `--prefer-family=ipv4` will force `wget` to use IPv4 for all transfers, while `--prefer-family=ipv6` will use IPv6 instead. This can help ensure consistent and reliable downloads in complex network environments.'
        preserve-permissions: '[AI] `wget --preserve-permissions` is a command-line option used with the Wget downloader tool. It instructs Wget to preserve the file permissions of the downloaded files, meaning it will maintain the original file permission bits during the transfer. This can be useful when downloading files from remote servers while ensuring they retain their intended access levels (read, write, execute) after being saved locally.'
        private-key: '[AI] `wget` is a command-line utility for downloading files from the web using various protocols, including HTTP and HTTPS. The option `--private-key <file>` is used when accessing an HTTPS server with SSL certificates signed by your own private key. In this context, `<file>` refers to the path of the private key file on your local system. By providing a private key, wget can authenticate and establish a secure connection with the server, making it suitable for accessing restricted resources or websites that require client-side SSL certificates for authentication.'
        private-key-type: '[AI] The `--private-key-type` option in the `wget` command is used to specify the type or format of a private key file that is being used for secure (HTTPS) downloads. This option helps `wget` correctly handle and utilize the provided private key file when it is required to authenticate and establish an encrypted connection with the remote server. Commonly used values for `--private-key-type` include "pkcs1" or "openssl-pkey".'
        progress: '[AI] `wget --progress` is a command-line option used with the Wget download tool. It enables the display of progress during downloads, providing real-time feedback on the transfer speed and total size of the file being retrieved. This option enhances the user experience by allowing you to monitor the download process in detail.'
        protocol-directories: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--protocol-directories` tells `wget` to follow links that point to directories (as opposed to files), and download the entire contents of those directories recursively. This means that when you use this option, `wget` will not only download the files in the specified directory but also any files in subdirectories, making it useful for downloading an entire website or FTP site.'
        proxy-password: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--proxy-password` is used to provide the password for an HTTP or HTTPS proxy server that `wget` should use when making requests. This can be useful in situations where you need to access content through a corporate or institutional proxy. The password is provided as an argument to this option, typically in plain text, but there are also ways to use a file containing the password instead.'
        proxy-user: |-
            [AI] The `--proxy-user` option in the `wget` command is used to specify a username for an HTTP or FTP proxy server. When you access remote resources using wget, and your network requires the use of a proxy server, this option allows you to authenticate yourself with the correct username, enabling successful downloads. For instance, if you have a proxy address `proxy.example.com` and its username is `user`, then you can use the command:

            ```bash
            wget --proxy=proxy.example.com --proxy-user=user <URL>
            ```

            Make sure to replace `<URL>` with the URL of the resource you want to download.
        quiet: '[AI] The `wget` command with the `--quiet` option is used to download files from the web while suppressing progress messages during the transfer process. This results in a quiet or silent download, making the output of the terminal focus more on other tasks rather than the download status updates.'
        quota: '[AI] The `--quota` option in `wget`, a popular command-line tool for downloading files from the web, sets a limit on the number of files that can be downloaded from a single FTP (File Transfer Protocol) or HTTP (Hypertext Transfer Protocol) server. When this limit is reached, further downloads from the same server will be skipped. This feature helps to avoid potential overloading of the server and simplifies managing multiple concurrent downloads.'
        random-wait: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--random-wait` introduces a random delay between each request, helping to avoid overwhelming the server with too many requests in a short time. This can be useful when downloading multiple files from a site that has bandwidth or resource limitations, as it mimics the behavior of a user accessing the site interactively rather than using automated tools. The delay is specified in seconds and is added to a uniform distribution between 0 and the given value.'
        read-timeout: '[AI] The `--read-timeout` option in the command-line tool `wget` sets a time limit for reading data from a server during a download. If wget does not receive any data from the server within this specified time, it will consider the download failed. This can be useful to prevent long-running or unresponsive downloads and improve overall performance in situations where network connectivity may be unstable or intermittent. The argument for this option is a time in seconds, for example `--read-timeout=10` sets a 10 second read timeout.'
        recursive: '[AI] `wget --recursive` is a command-line option used with the WGET utility to download files from the web. The `--recursive` or `-r` flag tells WGET to download not only the requested file but also all the files linked from that file recursively, following symlinks and descending into subdirectories. It is a powerful tool for mirroring websites, but it should be used with caution as it can consume significant network bandwidth and disk space.'
        referer: '[AI] The `--referer` option in `wget`, a popular command-line tool for downloading files from the web, sets the HTTP referer header sent with each request made by `wget`. The referer header specifies the URL of the page that links to the current resource being requested. This information can be used by servers to determine the source of traffic and implement access control or analytics. Setting a specific referer can help maintain consistency with other requests made from the same site, or bypass certain access controls based on the referer.'
        regex-type: '[AI] `wget` is a popular command-line tool for downloading files from the web. The option `--regex-type=extended` or simply `--regex` is used to specify a regular expression pattern for selecting the files to download based on their filenames. With this option, wget supports Perl Compatible Regular Expressions (PCRE) which allows more complex filename matching patterns than basic regex. The regular expression can be used with various file name components such as `-r`, `--accept`, and `--reject`. For example, `wget --regex="\.pdf$" http://example.com/` would download all files ending with `.pdf` from the given URL.'
        reject: '[AI] The `--reject` option in `wget`, a command-line tool for downloading files from the web, specifies a list of file name suffixes that wget will not download. When this option is used, wget will skip files with those suffixes during a recursive download (when using `-r` or `-R` options), and it will also not create local directories that match those suffixes. This can be useful for excluding specific types of files that are not needed in a download, such as images or scripts.'
        reject-regex: '[AI] The `--reject-regex` option in the `wget` command is used to reject downloading files whose names match a given regular expression pattern. This is useful when you want to download specific files but exclude others based on their names. For example, `wget --accept-referer=*.html --reject-regex=".*css"` will download all HTML files but not any CSS files. The regular expression should be enclosed in quotes and can include various regex metacharacters to make the pattern more complex if needed.'
        rejected-log: '[AI] `wget` is a command-line tool for downloading files from the web. The `--rejected-log` option is used to save a log file of URLs that were attempted to be downloaded but were rejected by wget due to various reasons such as server errors or inaccessible files. The log file contains detailed information about each rejected URL, including the HTTP status code and error message. This option helps in identifying and troubleshooting issues with downloading specific files from a web server.'
        relative: '[AI] The `--relative` option in `wget`, a popular command-line tool for downloading files from the web, changes the behavior of the output file path produced by relative URLs. When this option is used, wget generates file paths that are relative to the base directory where the index file or the current working directory is located, instead of creating absolute file paths based on the server''s URL structure. This can be useful for maintaining consistency between downloaded files and your local project directory organization.'
        remote-encoding: '[AI] The `--remote-encoding` option in `wget`, a popular command-line tool for downloading files from the web, specifies the expected encoding of the remote file content. By providing this information to wget, it can adjust its own internal data handling accordingly and save the file correctly, helping ensure the integrity and accuracy of the downloaded data.'
        report-speed: '[AI] The `wget` command is a popular tool for downloading files from the web using the command line. The option `--report-speed` is used to display the real-time download speed during the file transfer process. It helps users monitor the progress of their downloads and gives an estimate of how long it may take to complete. This option provides a more interactive experience, allowing users to identify any potential network issues or slow connections that may impact their download speed.'
        restrict-file-names: |-
            [AI] The `--restrict-file-names` option in `wget`, a popular command-line downloader, is used to restrict how wget names the files it downloads. By default, wget keeps the original file name for each remote file. However, with `--restrict-file-names`, you can set rules to modify the file names during download based on certain patterns or conditions:

            1. `--no-clobber`: Prevents overwriting of existing local files when downloading new files with the same name.
            2. `--rename-accepted`: Accepts and applies renaming rules specified with `--rename`.
            3. `--dont-rename-listings`: Does not apply renaming rules to index files or directories, but applies them to individual files.
            4. With the `--restrict-file-names=<regex>` option, you can use regular expressions to define custom naming patterns for downloaded files based on the remote file names or URLs.
        retr-symlinks: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--retr-symlinks` tells `wget` to follow and download symbolic links instead of skipping them, as is the default behavior. Symbolic links are special files that act as shortcuts to other files or directories in a file system. By using this option, `wget` will not only download the linked content but also create a local symbolic link pointing to it with the same name and location as the original one.'
        retry-connrefused: '[AI] `wget` is a command-line utility for downloading files from the web. The option `--retry-connrefused` tells `wget` to keep trying to connect to the server if the connection is refused due to a temporary condition, such as network congestion or server overload. This can be useful when attempting to download large or popular files that may temporarily exceed the server''s capacity. The flag also allows for configurable retries (default is 10) and time delay between retries in seconds.'
        retry-on-http-error: '[AI] The `wget` command is a popular tool used to download files from the web. The option `--retry-on-http-error` tells `wget` to automatically retry downloading a file if it receives an HTTP error code during the download process. This can be useful when dealing with unstable or intermittently available servers, as it allows `wget` to keep trying to download the file until it is successfully transferred.'
        save-cookies: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--save-cookies file.txt` instructs `wget` to save cookies (small pieces of data that store information about your browsing session) to the specified file after making an HTTP request. This can be useful when automating interactions with websites that require login or other stateful behavior.'
        save-headers: '[AI] The `wget` command is a popular tool for downloading files from the web using the command line. The option `--save-headers` tells `wget` to save the HTTP headers of the requested file along with the file itself when it is being downloaded. HTTP headers provide metadata about the file, such as its content type and encoding, size, and last modification date. So, using `--save-headers` option allows you to keep a record of this information alongside the downloaded file.'
        secure-protocol: '[AI] The `--secure-protocol` option in the `wget` command is used to specify the SSL/TLS protocol version for secure connections. By default, `wget` uses the first available SSL/TLS protocol. However, you can use this option to force `wget` to use a specific protocol, such as TLSv1.2 or SSLv3. This can be important for security reasons, especially when dealing with servers that may support insecure protocols. Using `--secure-protocol` helps ensure that your downloads are secure and encrypted using a supported and up-to-date protocol.'
        server-response: '[AI] `wget` is a command-line utility used to download files from the web. The `--server-response` option is used to display the server''s response during the download process. This can be useful for debugging or understanding what is happening during the transfer, such as checking for redirections, content negotiation, or other HTTP features. With this option, wget will print out detailed information about the headers and status codes received from the server.'
        show-progress: '[AI] The `--show-progress` option in `wget`, a command-line tool for downloading files from the web, displays progress bars and percentage completions while downloading files. It helps users monitor the download process in real time and provides feedback on the transfer speed and estimated time remaining.'
        span-hosts: '[AI] `wget` is a command-line tool for downloading files from the web using HTTP, FTP, and other protocols. The option `--save-host-rules=filename` or shorthand `--span-hosts=filename`, when used with `wget`, allows specifying a file containing rules to determine which hosts'' files should be saved during a mirroring operation. Each line in the rules file can contain a hostname, a regular expression matching hostnames, or an access control list (ACL) of IP addresses and netmasks, followed by "Allow" or "Deny". If no such file is specified, `wget` will download files from all hosts by default.'
        spider: '[AI] `wget --spider` is a command-line option for the WGET utility, which is a popular downloader tool. The `--spider` or `-s` option instructs WGET to perform a "crawl" of a website without actually downloading any files. Instead, it retrieves only the meta information about the remote pages and follows links on those pages, recursively exploring the site structure for indexing or reconnaissance purposes. It is often used in web development, SEO analysis, or for creating local mirrors of websites without the need to store the content.'
        start-pos: '[AI] The `--start-file` option in `wget`, a command-line download tool, specifies a file that contains URLs or ranges of URLs to start downloading from, instead of starting from the first URL in the given list. This can be useful when resuming interrupted downloads or downloading multiple files with partial content already available. The syntax is `--start-file=<filename>`.'
        strict-comments: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--strict-comment` tells `wget` to only accept and follow HTTP or FTP headers that include the "Content-Type" field, ignoring any other comments in the response. This can be useful when downloading files where incorrect or misleading comments in the server''s response might cause issues, such as when dealing with malformed or nonstandard web servers.'
        timeout: '[AI] `wget --timeout` is a command-line option used with the download utility ''wget''. It sets the maximum time allowed for a single operation, such as downloading a file or following a link, before giving up and reporting an error. The timeout value is specified in seconds. For example, `--timeout=10` would set a timeout of 10 seconds per operation. If no data is received within that time, wget will terminate the transfer and move on to the next task or file.'
        timestamping: '[AI] The `--timestamping` option in the `wget` command is used to add HTTP headers `If-Modified-Since` and `If-Unmodified-Since` with the last-modified timestamp of the local file, or zero if the local file does not exist. This allows the server to determine whether the locally cached content is up-to-date or needs to be re-downloaded. It helps in efficient caching and reducing network traffic for recurring downloads.'
        tries: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--tries=NUM` specifies the number of times (up to NUM) that `wget` will attempt to download a file if an error occurs during the process. By default, `wget` retries errors only once. Increasing the number of retries can improve the chances of successful downloads in case of transient errors or network instability. However, it may also increase the total download time and resource usage.'
        trust-server-names: '[AI] The `--trust-server-names` option in the `wget` command is used to trust the server''s reported name for last-modified timestamp and content-length checks during a retrieval. By default, `wget` performs these checks against its own local cache and not the server''s report. However, if the server is reliably reporting accurate information, this option can be used to save time and avoid unnecessary redownloads of previously successfully fetched files.'
        unlink: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--unlink` or `--delete` instructs `wget` to remove the local file, if it exists before starting the download. In summary, `wget --unlink <URL>` will download the file from the URL and delete any existing local file with the same name before doing so.'
        use-askpass: '[AI] `wget` is a popular command-line tool for downloading files from the web. The option `--use-askpass` is used to tell `wget` to interactively prompt for a password when necessary, instead of using the saved credentials in an insecure way. This can be useful when accessing protected resources that require authentication but where the password should not be stored in plain text or shared with other users on the system.'
        user: |-
            [AI] The `--user` option in the `wget` command is used to specify a custom user name for basic authentication during a file download. When accessing a protected resource over HTTP or FTP, `wget` may require this parameter to authenticate successfully. The argument is the username, and it should be provided as a string. For instance:

            ```bash
            wget --user=myusername --password=mypassword http://protected.server.com/file.ext
            ```
        user-agent: '[AI] `wget` is a command-line utility used to download files from the web. The option `--user-agent` allows users to specify a custom User-Agent string for the HTTP request header. A User-Agent string identifies the software and version being used in the request, providing transparency and helping websites to optimize their content accordingly. Misusing or falsifying User-Agent strings may violate some website terms of service or ethical guidelines.'
        verbose: '[AI] The `--verbose` option in the `wget` command is used to increase the amount of output displayed during a download. This can be helpful for debugging or monitoring the progress of the download. With this option, wget will print detailed information about each file being downloaded, including the HTTP headers and transfer progress. Additionally, it provides more information on any errors that may occur during the download process.'
        version: '[AI] `wget --version` is a command used in Unix-based systems to check the version information of the installed wget utility, which is a popular command-line tool for downloading files over the internet using HTTP, FTP, and other protocols. The output of this command will display the specific version number, compilation options, or any other relevant information about the installed wget package.'
        wait: '[AI] The `--wait` option in `wget`, a popular command-line tool for downloading files from the web, instructs it to pause and wait between retrieval of files or after hitting an HTTP error status code before trying again. This can be useful when dealing with websites that have certain access restrictions or require specific intervals between requests to avoid overloading or being blocked. The `--wait` option accepts a time argument in seconds. For example, `wget --wait=5 URL` would make `wget` wait for 5 seconds before attempting the next download or request.'
        waitretry: '[AI] The `--waitretry` option in `wget`, a popular command-line downloader, instructs it to automatically retry downloading a failed HTTP or FTP request after a specified interval. By default, `wget` waits for 5 seconds before retrying, but this interval can be adjusted using the `--waitretry=SECONDS` argument. The purpose of this option is to help overcome temporary network connectivity issues or overloaded servers, allowing the download to resume eventually without manual intervention.'
        warc-cdx: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--warc-cdx` tells `wget` to create a Warc-CDX file instead of downloading regular files. Warc-CDX is an archival format used in the Web Archive Collection Exchange (WACE) for describing and interoperably sharing collections of web pages and other resources. The option generates a CDX file, which contains metadata about the downloaded URLs, such as their status codes, sizes, and content types. This can be useful for large-scale web archiving projects or for analyzing the contents of a website over time.'
        warc-dedup: '[AI] The `wget` command with the option `--warc-dedup` is used to download files from the web and save them in the Warc format, which is a standardized archive format for web data. The `--warc-dedup` option ensures that only unique URLs are saved in the Warc file, preventing duplicate entries. This can be useful when crawling websites or collecting data from multiple sources to reduce redundancy and save storage space.'
        warc-file: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--warc-file=FILE.warc.gz` tells `wget` to save the downloaded data in the Warc format, which is a standardized archival format used for Web data. This option outputs the downloaded data into a single gzipped WARC file named `FILE.warc.gz`, preserving metadata such as URLs, timestamps, and HTTP headers. It''s often used for web archiving or scraping projects.'
        warc-header: '[AI] `wget --warc-header` is a command-line option used with the wget utility, which is a popular tool for downloading files from the web. This specific option causes wget to generate a WARC (Web ARChive) file instead of the usual local copy of the downloaded content. A WARC file is a standardized format for archiving web data, including metadata such as HTTP headers and timestamps. The `--warc-header` option sets custom headers that will be included in the WARC file''s metadata.'
        warc-max-size: '[AI] `wget` is a popular command-line tool for downloading files from the web. The option `--warc-max-size` sets a limit on the total size of Warc files generated by `--save-headers --output-document=-`. Warc (Web Archive) is a format used to store and archive web pages, including metadata like HTTP headers. With `--warc-max-size`, you can specify a maximum size in bytes for the generated Warc file(s). If the combined size of the Warc files exceeds this limit, `wget` will stop downloading additional content. This is useful when you want to limit the amount of data being saved or archived.'
        warc-tempdir: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--warc-tempdir=<directory>` is used to specify a directory where wget should store temporary WARC (Web Archive) files while downloading. These files contain metadata and the contents of the downloaded resources, including HTTP headers and the resource content itself. This option can be useful for large-scale web scraping or archiving projects.'
        xattr: '[AI] `wget` is a command-line tool for downloading files from the web. The option `--xattr` enables extended attributes (also known as file metadata) handling during file transfers. This can be useful for preserving certain metadata when downloading files, such as file access and modification times, or security labels. By default, wget does not preserve extended attributes, so using this option ensures that the downloaded file''s metadata matches the original file on the web server.'
